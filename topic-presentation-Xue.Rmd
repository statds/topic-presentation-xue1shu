---
title: "Web Scraping using R"
author: "Yishu Xue"
date: "03/01/2018"
output:
  ioslides_presentation: default
  beamer_presentation: default
  slidy_presentation: default
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(message = FALSE, warning = FALSE)
pkgs <- c("stringr", "twitteR", "wordcloud", "tidyr", "SnowballC",
          "purrr", "dplyr", "plotly", "tidytext")
for (i in 1:length(pkgs)){
    if (! pkgs[i] %in% installed.packages()){
        install.packages(pkgs[i], dependencies = TRUE,
                         repos = "https://cloud.r-project.org")
    }
}
library(stringr)
library(SnowballC)
library(dplyr)
library(purrr)
library(twitteR)
library(MASS)
library(ggplot2); library(plotly)
library(wordcloud)
library(tidytext)
```

## What is web scraping?


Web scraping is a technique for converting the data present in unstructured
format (HTML tags) over the web to structured format which can easily be accessed
and used. The job is carried out by a piece of code which is called a "scraper". 

First, it sends a “GET” query to a specific website. Then, it parses an HTML 
document based on the received result. After it’s done, the scraper searches 
for the data you need within the document, and, finally, converts it into 
whatever specified format.

## Web crawling vs web scraping: what are the differences?

###  Web crawling

Web crawling, means the usage of programs or automated scripts which browse the 
World Wide Web in a methodical, automated manner. Google, Bing and Yahoo,
essentially, are the major web crawlers. Crawlers can be used to gather specific 
types of information from Web pages, such as harvesting e-mail addresses 
(usually for spam). [Example 1](https://imgur.com/a/aX4yX) 
[Example 2](http://merlot.stat.uconn.edu/~jyan/)

## Web crawling vs web scraping: what are the differences?

### Web scraping

Web scraping is also same as finding required data from web pages. But in 
scraping, we know the exact page from which we need to scrap the data. 
We also know the html element structure of the webpages as they are fixed. Compared
to web crawling, it is more targeted. 


##  Why do we need web scraping?
Web scraping has immense possibilities in application. Examples are:

- Scraping movie rating data from IMDB to create movie recommendation engines
- Scraping text data from Wikipedia and other sources to make NLP-based systems
- Scraping labeled image data from websites like Google, Pinterest, Flickr, etc. to 
train image classification models
- Scraping data from social media sites like Facebook and Twitter for 
performing sentiment analysis, oponion mining, etc.
- Scraping user reviews and feedbacks from e-commerce sites like Amazon



## Ways to scrap data

- **Human copy-paste**: slow and inefficient
- **Text pattern matching**: use regular expression matching 
- **API interface**: many websites Facebook and Twitter provides public/private
APIs which can be called using standard code to retrive data in prescribed 
format
- **DOM parsing**: by using the web browsers, programs can retrive the dynamic content 
generated by client-based-scripts. It's also possible to parse web pages into a 
DOM tree, based on which programs can retrive parts of these pages

## Packages for web scraping in R

- `httr`: provides a user-friendly interface for executing HTTP methods

- `RCurl`: provides a closer interface between `R` and the `libcurl` `C` library, but less 
user-friendly

- `curl`: another `libcurl` client; provides the `curl()` function as an SSL-compatible 
replacement for base R's `url()` and support for http 2.0, ssl, gzip, deflate and more

- `request`: provides a high-level package that is useful for developing 
other API client packages


## Packages for web scraping in R

- `RSelenium`: automate interactions and extract page contents of dynamically generated 
webpages (those requiring user interaction to display results)

- `Rcrawler`: performs parallel web crawling and web scraping; designed to crawl,
parse and store web pages to produce data that can be directly used for 
analysis application. 

- `rvest`: a higher-level alternative package useful for web scraping; 
works with `magrittr` 
to make it easy to express common web scraping tasks

- `twitteR`: an `R` based Twitter client that procides an interface to the Twitter web API


For more information: [CRAN Task View - Web Technologies and Services](https://cran.r-project.org/web/views/WebTechnologies.html)

## Example 1: IMDB Movies

Let's scrap the IMDB website for the 100 most popular feature films released in 
2017. The website is 
[here](http://www.imdb.com/search/title?count=100&release_date=2017,2017&title_type=feature).

```{r}
library(rvest)
# specify the url for desired website to be scrapped
url <- "http://www.imdb.com/search/title?count=100&release_date=2017,2017&title_type=feature"

# read in the HTML code from the website
webpage <- read_html(url)
```

## Example 1: IMDB Movies

### What does the page contain? 

* Rank: the rank of the film by popularity

* Title: the title of the feature film

* Description: the description of the film

* Runtime: the duration of the film

* Genre: the genre of the feature film

* Rating: the IMDB rating of the feature film

* Metascore: the metascore on IMDB website for the feature film

## Example 1: IMDB Movies

* Votes: votes cast in favor of the feature film

* Gross_Earning_in_Mil: gross earnings of the feature film in millions

* Director: The main director of the feature film. In cases where there are 
multiple directors, we take the first one.

* Actor: The main actor/actress of the feature film. In cases where there are 
multiple actors/actresses, we take the first one.

## Example 1: IMDB Movies

### Start with the rankings

```{r}
# rankings
rank_data_html <- html_nodes(webpage, ".text-primary")
rank_data <- as.numeric(html_text(rank_data_html))
# names
title_data_html <- html_nodes(webpage, ".lister-item-header a")
title_data <- html_text(title_data_html)

head(data.frame(title_data, rank_data))
```

## Example 1: IMDB Movies

 Similarly, identify the Description, Runtime, Genre, Rating, Metascore, 
 Votes, Gross Earning, Director and Actor

```{r}
description_data_html <- html_nodes(webpage, 
                        ".ratings-bar+ .text-muted")
description_data <- html_text(description_data_html)

head(description_data)
```

## Example 1: IMDB Movies

```{r}
runtime_data_html <- html_nodes(webpage, ".text-muted .runtime")
runtime_data <- html_text(runtime_data_html)
head(runtime_data)
runtime_data <- as.numeric(gsub(" min","", runtime_data))
```

```{r, echo = FALSE}
genre_data_html <- html_nodes(webpage, ".text-muted .genre")
genre_data <- html_text(genre_data_html)
genre_data <- gsub("\n", "", genre_data)
genre_data <- gsub(" ", "", genre_data)
genre_data <- gsub(",.*", "", genre_data)
genre_data <- as.factor(genre_data)

rating_data_html <- html_nodes(webpage, ".ratings-imdb-rating strong")
rating_data <- as.numeric(html_text(rating_data_html))

votes_data_html <- html_nodes(webpage, ".sort-num_votes-visible span:nth-child(2)")
votes_data <- as.numeric(gsub(",", "", html_text(votes_data_html)))

directors_data_html <- html_nodes(webpage,'.text-muted+ p a:nth-child(1)')
directors_data <- html_text(directors_data_html)
directors_data<-as.factor(directors_data)

actors_data_html <- html_nodes(webpage,'.lister-item-content .ghost+ a')
actors_data <- html_text(actors_data_html)
actors_data<-as.factor(actors_data)
```

## Example 1: IMDB Movies

Metascore: what goes wrong?

```{r}
metascore_data_html <- html_nodes(webpage,'.metascore')
metascore_data <- html_text(metascore_data_html)
metascore_data<-gsub(" ","",metascore_data)
length(metascore_data)
```

## Example 1: IMDB Movies

Six of the top 100 movies didn't have a metascore!

```{r}
for (i in c(11, 29, 33, 39, 59, 85)) {
    a <- metascore_data[1:(i - 1)]
    b <- metascore_data[i:length(metascore_data)]
    metascore_data <- append(a, list("NA"))
    metascore_data <- append(metascore_data, b)
}
metascore_data <- as.numeric(metascore_data)
```

```{r, echo=FALSE}
gross_data_html <- html_nodes(webpage, '.ghost~ .text-muted+ span')

#Converting the gross revenue data to text
gross_data <- html_text(gross_data_html)
gross_data <- gsub("M", "", gross_data)
gross_data <- gsub("$", "", gross_data)
gross_data <- substring(gross_data, 2, 6)

for (i in c(11, 15, 23, 29, 33, 35, 39, 43, 50, 59, 63, 71, 75, 84, 85, 89, 98)) {
    a <- gross_data[1:(i - 1)]
    b <- gross_data[i:length(gross_data)]
    gross_data <- append(a, "NA")
    gross_data <- append(gross_data, b)
}

#Data-Preprocessing: converting gross to numerical
gross_data <- as.numeric(unlist(gross_data))
```

## Example 1: IMDB Movies

Combine everything into a single dataframe.

```{r, echo = FALSE}
movies_df <- data.frame(
    Rank = rank_data,
    Title = title_data,
    Description = description_data,
    Runtime = runtime_data,
    Genre = genre_data,
    Rating = rating_data,
    Metascore = metascore_data,
    Votes = votes_data,
    Gross_Earning_in_Mil = gross_data,
    Director = directors_data,
    Actor = actors_data
    )
str(movies_df)
```

## Example 1: IMDB Movies

<!-- An overall visualization of movie runtime, rating and their votes by genre. -->

```{r, message = FALSE}
p1 <- ggplot(movies_df, aes(x = Runtime, y = Rating)) + 
    geom_point(aes(size = Votes, col = Genre)) 
ggplotly(p1)
```

## Example 1: IMDB Movies

```{r, message = FALSE}
ggplotly(ggplot(movies_df, aes(x = Runtime, fill = Genre)) + 
             geom_bar(stat = "count"))
```

## Example 2: Twitter Data

```{r, echo = FALSE}
consumer_key <- "uvTWKchcoq3fZWBXIhisojF2R"
consumer_secret <- "JGJ2IPr0ywipMWSx4zBSBoFYRjRSnjrDIGzsCqBs1VA92pOFPG"
access_token <- "798334926101876737-7zCBIogEBs2hoO3f7plquYEl717Xvs2"
access_secret <- "YhEn93E3oXt72MbexLCOCxmqwr7c5Q7XE7nGsTpntkQ4R"
```

Initial setup: generate `consumer_key`, `consumer_secret`, `access_token`,
`access_secret` from [Twitter Application Management](https://apps.twitter.com),
and use them to authorize your `R` session.

```{r, message = FALSE}
library(twitteR)
setup_twitter_oauth(consumer_key, consumer_secret,
                    access_token, access_secret)
```

## Example 2: Twitter Data

```{r}
searchResults <- searchTwitteR("#coco", n = 100)
head(searchResults)
```

## Example 2: Twitter Data

```{r}
searchResults <- searchTwitteR("#coco", n = 3000, lang = "en")
head(searchResults)
```

## Example 2: Twitter Data

```{r, echo = FALSE}
reg <- "([^A-Za-z\\d#@']|'(?![A-Za-z\\d#@]))"
tweets <- tbl_df(map_df(searchResults, as.data.frame))
tweet_words <- tweets %>%
  filter(!str_detect(text, '^"')) %>%
  mutate(text = str_replace_all(text, "https://t.co/[A-Za-z\\d]+|&amp;", "")) %>%
  mutate(text = str_replace_all(text, "@[A-Za-z\\d]+|&amp;", "")) %>%
  unnest_tokens(word, text, token = "regex", pattern = reg) %>%
  filter(!word %in% stop_words$word,
         str_detect(word, "[a-z]"))
words <- tweet_words %>% group_by(word) %>% summarise(count = n()) %>% arrange(desc(count))
freqWords <- as.data.frame(words)[-which(words$word %in% c("rt", "#coco")),]
freqWords2 <- freqWords[1:30, ]

m <- list(
   l = 120,
   r = 20,
   b = 20,
   t = 20,
   autoexpand = TRUE
 )

plot_ly(freqWords2, y = ~word, x = ~count, type = "bar",
        orientation = "h") %>% layout(autosize = T,
                                      margin = m,
                                      yaxis = list(title = "", tickfont =
                                                       list(size = 10)))

```

## Example 2: Twitter Data

```{r, echo=FALSE}
wordcloud(freqWords$word,freqWords$count, min.freq = 3, 
           max.words=300, random.order=TRUE, rot.per=0.35,
           colors=brewer.pal(12, "Paired"))
```

## One more thing: word cloud about Donald Trump

```{r, echo=FALSE}
reg <- "([^A-Za-z\\d#@']|'(?![A-Za-z\\d#@]))"
searchResults <- searchTwitteR("#donaldtrump", n = 3000, lang = "en")
#head(searchResults)
tweets <- tbl_df(map_df(searchResults, as.data.frame))
tweet_words <- tweets %>%
  filter(!str_detect(text, '^"')) %>%
  mutate(text = str_replace_all(text, "https://t.co/[A-Za-z\\d]+|&amp;", "")) %>%
      mutate(text = str_replace_all(text, "@[A-Za-z\\d]+|&amp;", "")) %>%
  unnest_tokens(word, text, token = "regex", pattern = reg) %>%
  filter(!word %in% stop_words$word,
         str_detect(word, "[a-z]"))
words <- tweet_words %>% group_by(word) %>% summarise(count = n()) %>% arrange(desc(count))
freqWords <- as.data.frame(words)[-which(words$word %in% c("rt", "#donaldtrump",
                                                           "#trump",
                                                           "@realdonaldtrump")),]
wordcloud(freqWords$word,freqWords$count, min.freq = 3, 
           max.words=300, random.order=TRUE, rot.per=0.35,
           colors=brewer.pal(12, "Paired"))
```

## For large, formulated websites: customized packages

`Rfacebook`, `Rlinkedin`, `RGoogleData`, `googlesheets`

These packages are very similar to `twitteR`, as they all require specific API's 
from the websites. You'll need to create a web app.

[facebook](https://developers.facebook.com/apps)

[linkedin](https://developer.linkedin.com/)

[googlesheets, RGoogleData](https://console.developers.google.com/apis/dashboard?project=groovy-form-88400&duration=PT1H)

## More reading:

[Rcrawler: An R package for parallel web crawling and scraping](https://www.sciencedirect.com/science/article/pii/S2352711017300110)

[UC Business Analytics R Programming Guide: Scraping Data](http://uc-r.github.io/scraping)

[Examples of Web Scraping With R](http://www.programmingr.com/examples/examples-web-scraping-r/)

[Conducting Web-based Research in R](http://www.columbia.edu/~cjd11/charles_dimaggio/DIRE/styled-4/styled-6/code-13/)